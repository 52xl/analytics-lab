corrplot(cor(data[,-4]),
method = "number",
type = "upper") # show only upper side
#look at data relationships
x11() # x11() for Windows 11 users
# quartz() for Mac users
pairs(~ medv + lstat + rm + ptratio, data = data, main = "Housing Data")
plot(medv~lstat, data)
plot(medv~rm, data)
plot(medv~crim, data)
plot(medv~ptratio, data)
# Data Partition (in case you do not perform cv)
set.seed(123) # use set.seed and the number you most like for reproducibility
index <- sample(2, nrow(data), replace = T, prob = c(0.75, 0.25))
train <- data[index==1,] # training data
nrow(train) / nrow(data) # training data = 76% of original data
test <- data[index==2,] # test data
nrow(test) / nrow(data) # test data = 24% of original data
## Cross Validation (CV)
# Custom Control Parameters inside caret package for CV
custom <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T)
set.seed(123) # replicability
slm <- train(medv~rm,
data,    #using cross-validation we could use all data as the training data
method ='lm',
trControl=custom)
slm
summary(slm)
lm #check error and model conditions
# RMSE is calculated with the remaining data points of the CV
# i.e., in this case the 10th fold of data, that is sometimes 49 points, sometimes 50 points
summary(lm) #look at coefficients
# RMSE is calculated with the remaining data points of the CV
# i.e., in this case the 10th fold of data, that is sometimes 49 points, sometimes 50 points
summary(lm) #look at coefficients
## Multiple Linear Model
# median value of owner-occupied homes in USD 1000's
# function of all other variables
set.seed(123) # replicability
lm <- train(medv~.,
data,
method ='lm',
trControl=custom)
lm #check error and model conditions
# RMSE is calculated with the remaining data points of the CV
# i.e., in this case the 10th fold of data, that is sometimes 49 points, sometimes 50 points
summary(lm) #look at coefficients
# Results - normality assumptions
# Did I apply the linear model under the necessary assumptions?
x11() # x11() for Windows 11 users
# quartz() for Mac users
par(mfrow=c(2,2))
plot(lm$finalModel)
## Multiple Linear Model
# median value of owner-occupied homes in USD 1000's
# function of
# only significant variables from previous model
set.seed(123) # replicability
lm2 <- train(medv~.,
data[,-c(3,7)],
method ='lm',
trControl=custom)
lm2
summary(lm2)
#graphic setting
colors<-c(brewer.pal(8,"YlGnBu"),brewer.pal(8,"RdPu"))
colors<-colors[c(-1,-9)]
# Ridge and Lasso require scaled values
cols <- c("crim","zn","indus","nox","rm","age","dis","rad",
"tax","ptratio","b","lstat")
pre_proc_val <- preProcess(data[,cols],
method = c("center", "scale"))
nor_data <- predict(pre_proc_val, data[,cols])
nor_data$chas <- data$chas
nor_data$medv <- data$medv
summary(nor_data)
## Ridge regression
# median value of owner-occupied homes in USD 1000's
# function of all variables
set.seed(123) # replicability
ridge <- train(medv~.,
nor_data,
method ='glmnet',
tuneGrid = expand.grid(alpha=0,
# alpha = 0 for Ridge
# alpha = 1 for Lasso
lambda = seq(0.001, 1, length=5)
# lambda = regularization parameter
),
trControl=custom)
ridge #check lambda values and RMSE
# plot Results
#x11() for Windows 11 users
x11() #quartz() for Mac users
plot(ridge)
x11() #quartz() for Mac users
plot(ridge$finalModel, xvar = 'dev', label=T,col=colors[1:13]) #predictors that explain the variability
legend(x="topleft",legend=ridge$coefnames[1:7],lty=1,col = colors[1:7],cex=0.8,bty="n")
legend(x=0.12,y=3.18,legend=ridge$coefnames[8:13],lty=1,col = colors[8:13],cex=0.8,bty="n")
x11()
plot(varImp(ridge, scale=T))
x11() #quartz() for Mac users
plot(ridge$finalModel, xvar = 'dev', label=T,col=colors[1:13]) #predictors that explain the variability
legend(x="topleft",legend=ridge$coefnames[1:7],lty=1,col = colors[1:7],cex=0.8,bty="n")
legend(x=0.12,y=3.18,legend=ridge$coefnames[8:13],lty=1,col = colors[8:13],cex=0.8,bty="n")
x11()
plot(varImp(ridge, scale=T))
ridge #check lambda values and RMSE
x11() #quartz() for Mac users
plot(ridge)
x11() #quartz() for Mac users
plot(ridge$finalModel, xvar = 'dev', label=T,col=colors[1:13]) #predictors that explain the variability
legend(x="topleft",legend=ridge$coefnames[1:7],lty=1,col = colors[1:7],cex=0.8,bty="n")
legend(x=0.12,y=3.18,legend=ridge$coefnames[8:13],lty=1,col = colors[8:13],cex=0.8,bty="n")
## Lasso Regression
# median value of owner-occupied homes in USD 1000's
# function of all variables
set.seed(123) # replicability
lasso <- train(medv~.,
nor_data,
method ='glmnet',
tuneGrid = expand.grid(alpha=1,
# alpha = 0 for Ridge
# alpha = 1 for Lasso
lambda = seq(0.001, 0.2, length=5)
# lambda = regularization parameter
),
trControl=custom)
# plot results
plot(lasso)
x11() # quartz() for Mac users
plot(lasso$finalModel, xvar = 'dev', label=T,col=c(2:13))#predictors that explain the variability
x11()
plot(varImp(lasso, scale=T))
## Compare all models
model_list <- list(LinearModel=lm,Ridge=ridge,Lasso=lasso)
res <- resamples(model_list) # from caret package
summary(res)
bwplot(res)
x11()
bwplot(res)
p1 <- predict(lm, test)
sqrt(mean((test$medv-p1)^2))
p2 <- predict(lm2, test)
sqrt(mean((test$medv-p2)^2))
# Best Model
lasso$bestTune
best <- lasso$finalModel
coef(best, s = lasso$bestTune$lambda)
# Best Model
lasso$bestTune
best <- lasso$finalModel
coef(best, s = lasso$bestTune$lambda)
#Read data
souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
#Read data
souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
#convert into a time series object
souvenir_ts<- ts(souvenir, frequency=12, start=c(1987,1))
souvenir_ts
#plot the time series
plot(souvenir_ts)
x11()
plot(souvenir_ts)
#the size of the fluctuation and the random fluctuations seem to increase with time
#a transformation could be useful
logsouvenir_ts <- log(souvenir_ts)
#the size of the fluctuation and the random fluctuations seem to increase with time
#a transformation could be useful
logsouvenir_ts <- log(souvenir_ts)
plot.ts(logsouvenir_ts)
#Decompose time series
logsouvenir_ts_comp <- decompose(logsouvenir_ts)
logsouvenir_ts_comp$seasonal
x11()
plot(logsouvenir_ts_comp)
logsouvenir_ts_comp$seasonal
x11()
plot(logsouvenir_ts_comp)
#estimate the exponential smoothing model
souvenir_tsmodel <- HoltWinters(logsouvenir_ts)
souvenir_tsmodel
souvenir_tsmodel$alpha
souvenir_tsmodel$beta #The value of beta 0 indicates that the estimate of the slope b of the trend component is not updated over the time series, and instead is set equal to its initial value.
souvenir_tsmodel$gamma #a high value of gamma indicates that the estimate of the seasonal component at the current time point is  based upon very recent observations.
souvenir_tsmodel$coefficients
#plot the model estimations
plot(souvenir_tsmodel)
x11()
plot(souvenir_tsmodel)
souvenir_ts_future<- forecast(souvenir_tsmodel,h=48)
x11()
plot(souvenir_ts_future)
require(forecast)
require(urca)
#forecast future values
souvenir_ts_future<- forecast(souvenir_tsmodel,h=48)
x11()
plot(souvenir_ts_future)
#check model assumptions
acf(souvenir_ts_future$residuals, lag.max=20, na.action = na.pass)
Box.test(souvenir_ts_future$residuals, lag=20, type="Ljung-Box")
plot.ts(souvenir_ts_future$residuals) # make a time plot of residuals
x11()
plot.ts(souvenir_ts_future$residuals) # make a time plot of residuals
#estimate the ARIMA model
#create a stationary time series
#remove seasonality
logsouvenir_ts_s_adj <- logsouvenir_ts - logsouvenir_ts_comp$seasonal
x11()
plot(logsouvenir_ts_s_adj)
#apply diff
diff_logsouvenir_ts <- diff(logsouvenir_ts_s_adj, differences=2)
#test stationarity
ur.kpss(logsouvenir_ts) #original log_ts is not stationary
ur.kpss(diff_logsouvenir_ts) #ts seasonality adj and with 'diff' is stationary
x11()
plot.ts(diff_logsouvenir_ts)
#define order of the auto-regressive part
x11()
acf(diff_logsouvenir_ts, lag.max=20) # plot a correlogram
acf(diff_logsouvenir_ts, lag.max=20, plot=F) # get the autocorrelation
#define order of the moving-average part
x11()
pacf(diff_logsouvenir_ts, lag.max=20) # plot a correlogram
pacf(diff_logsouvenir_ts, lag.max=20, plot=F) # get the autocorrelation
#define model parameters automatically
auto.arima(diff_logsouvenir_ts) #ARIMA (2,0,0)
#estimate the model
souvenir_arima <- arima(diff_logsouvenir_ts, order=c(2,0,0))
souvenir_arima
library(ISLR) #default data is here
library(dplyr)
library(ggvis)
library(boot)
require(ggplot2)
library(class)
data<-Default
#data partition
set.seed(123) # use set.seed for reproducibility
index <- sample(2, nrow(data), replace = T, prob = c(0.75, 0.25))
train <- data[index==1,] # training data
nrow(train) / nrow(data) # training data = 75% of original data
test <- data[index==2,] # test data
nrow(test) / nrow(data) # test data = 25% of original data
# estimate the logistic model
log_model<-glm(default~balance + student + income, family = "binomial", data = train)
#multinomail instead of binomial if your are facing more than two categories
summary(log_model)
## explore relation between students and balance
x11()
boxplot(balance~student,
data=train,
main="Student status and credit balance",
xlab="Student status",
ylab="Credit card balance")
# estimate the logistic model
log_model<-glm(default~balance + student + income, family = "binomial", data = train)
#multinomail instead of binomial if your are facing more than two categories
summary(log_model)
## explore relation between students and balance
x11()
boxplot(balance~student,
data=train,
main="Student status and credit balance",
xlab="Student status",
ylab="Credit card balance")
#predict test values
glm.probs = predict(log_model, newdata = test, type = "response")
glm.pred = ifelse(glm.probs > 0.5, "Yes", "No")
#explore a single example
test[1,]
glm.probs[1]
#explore a single - default example
glm.probs[glm.probs>0.5]
#evaluate model accuracy
metrics<-table(glm.pred, test$default)
mean(glm.pred==test$default) #overall error
#evaluate model precision
metrics[2,2]/(metrics[2,2]+metrics[1,2])
#evaluate model recall
metrics[2,2]/(metrics[2,2]+metrics[2,1])
#applied knn to the previous problem
#for euclidean distance all variables need to be numeric
levels(train$student)[levels(train$student)=="No"] <- "0"
levels(train$student)[levels(train$student)=="Yes"] <- "1"
levels(test$student)[levels(test$student)=="No"] <- "0"
levels(test$student)[levels(test$student)=="Yes"] <- "1"
train$student<-as.numeric(as.character(train$student))
test$student<-as.numeric(as.character(test$student))
#knn
knn_fit <- knn(train[,c(2:4)], test[,c(2:4)], train$default, k = 5)
summary(knn_fit)
metrics_knn<-table(knn_fit, test$default)
metrics_knn
mean(knn_fit==test$default)
#model precision
metrics_knn[2,2]/(metrics_knn[2,2]+metrics_knn[1,2])
#model recall
metrics_knn[2,2]/(metrics_knn[2,2]+metrics_knn[2,1])
# % of protein intakes for 9 food sources by country
my_data <- read.csv("~/../Desktop/protein.csv",
stringsAsFactors = FALSE)
# % of protein intakes for 9 food sources by country
my_data <- read.csv("~/../Desktop/protein.csv",
stringsAsFactors = FALSE)
# Which data
str(my_data)
# number of observations
n <- nrow(my_data)
# number of variables
p <- ncol(my_data)
# Which distributions
summary(my_data)
# Search for Italy
View(my_data)
# Details of Italy
my_data[which(my_data$Country=="Italy"),] # Italy
# Explore data
x11() # x11() for Windows 11 users # quartz() for Mac users
pairs(~ RedMeat + WhiteMeat + Eggs + Milk + Fish + Cereals +
Starch + Nuts + Fr.Veg,
data = my_data, main = "Protein")
pairs(RedMeat ~ WhiteMeat,
data = my_data, main = "Protein")
##### Hierarchical clustering #####
?stats::hclust
# d = dissimilarity
# method = linkage
hc <- hclust(dist(my_data))
# why is an NA generated?
# --> cannot handle categorical data!
hc <- hclust(dist(my_data[, -1]))
# Summary of clustering instructions
print(hc)
# Table describing merging
hc[[1]]
# 1st merge: observations 6 and 20
# 6th merge: observation 15 with cluster 1 (6&20)
x11()
plot(hc)
# The closest country to Italy in terms of protein intakes is...
# Greece!
my_data[10,]$Country
my_data[10,]
# At which step have they been merged?
hc[[1]]
which(hc[[1]][,1]==-13)
which(hc[[1]][,2]==-13)
# Other countries close to Italy in terms of protein intakes are:
my_data[23, ]$Country
my_data[1, ]$Country
# Which is the most distant country to Italy in terms of protein intake?
my_data[,]$Country
x11() # x11() for Windows 11 users # quartz() for Mac users
pairs(~ RedMeat + WhiteMeat + Eggs + Milk + Fish + Cereals + Starch + Nuts + Fr.Veg,
data = my_data, main = "Protein")
# Euclidean
dist_eucl <- stats::dist(my_data)
#dist_eucl <- stats::dist(my_data[, -1])
x11() # x11() for Windows 11 users # quartz() for Mac users
image(1:n,1:n,as.matrix(dist_eucl),
main='Euclidean Dissimilarity',
asp=1)
# Manhattan
dist_manh <- stats::dist(my_data[, -1], method="manhattan")
x11() # x11() for Windows 11 users # quartz() for Mac users
image(1:n,1:n,as.matrix(dist_manh),
main='Manhattan Dissimilarity',
asp=1)
plot(hc_complete)
hc_complete <- hclust(dist(my_data), method = "complete")
#hc_complete <- hclust(dist(my_data[, -1]), method = "complete")
x11()
plot(hc_complete)
hc_single <- hclust(dist(my_data[,-1]), method = "single")
x11()
plot(hc_single)
hc_average <- hclust(dist(my_data[, -1]), method = "average")
x11()
plot(hc_average)
hc_ward <- hclust(dist(my_data[, -1]), method = "ward.D2")
x11()
plot(hc_ward)
#### Model selection ("trust") - Cophenetic coefficients ####
?cophenetic
# - Take trace of the values of the dissimilarity at with you aggregate a statistical unit with another
# - Compute the correlation between the distance matrix and the cophenetic matrix
# => Plot coefficients of cophenetic matrix (original on x axis, C on y axis)
# OBSERVE: If the value C(i,j) is very far from the initial
# dissimilarity, the clustering algorithm is not capturing very good
# the reality; otherwise we can trust this algorithm!
# ELEMENT (I,J): height at which points i and j have been merged in a cluster
coph_s <- cophenetic(hc_single)
coph_c <- cophenetic(hc_complete)
coph_a <- cophenetic(hc_average)
coph_w <- cophenetic(hc_ward)
x11()
image(1:n,1:n,as.matrix(coph_s), main="Single", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_c), main="Complete", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_a), main="Average", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_w), main="Ward", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_s), main="Single", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_c), main="Complete", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_a), main="Average", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
x11()
image(1:n,1:n,as.matrix(coph_w), main="Ward", xlab="", ylab="", xlim = c(0,n+1), ylim=c(0,n+1))
cophCorr <- c(cor(dist_eucl, coph_s), cor(dist_eucl, coph_c), cor(dist_eucl, coph_a), cor(dist_eucl, coph_w) )
cophCorr
# Similarity of clustering distance matrices w.r.t. distance matrix of original data:
# c(cor(dist, coph_s), cor(dist, coph_c), cor(dist, coph_a), cor(dist, coph_w) )
### Cluster Quality ####
# How many clusters?
?cutree
# k number of clusters
# h height at which to cut
# output: memberships
# high cut
clust_2 <- cutree(hc, k=2)
table(clust_2)
# low cut
clust_11 <- cutree(hc, k=11)
table(clust_11)
# intermediate cut
clust_5 <- cutree(hc, k=5)
table(clust_5)
# Which countries are clustered together?
my_data[clust_5==1,]$Country
my_data[clust_5==2,]$Country
my_data[clust_5==3,]$Country
my_data[clust_5==4,]$Country
my_data[clust_5==5,]$Country
pairs(~ RedMeat + WhiteMeat + Eggs + Milk + Fish + Cereals +
Starch + Nuts + Fr.Veg,
data = my_data, main = "Protein",
col = clust_5)
transactions <- read.csv2("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Exercises/transactions.csv")
View(transactions)
db=data.frame(transactions)
View(db)
View(db)
library(dplyr)
Client= db %>% group_by(db$id_cust)
View(Client)
Client= db %>% group_by(db$id_cust,db$)  %>% summarise(total_sales = sum(db$gross_PL_sales), shops = count())
View(Client)
Client= db %>% group_by(db$id_cust)  %>% summarise(total_sales = sum(db$gross_PL_sales), shops = count())
View(Client)
Client= db %>% group_by(db$id_cust)  %>% count()
View(Client)
Client_shops= db %>% group_by(db$id_cust)  %>% count()
library(arules)
library(arulesViz)
library(datasets)
data=data(Groceries)
itemFrequencyPlot(Groceries,
topN=choose_the_number,type="absolute")
itemFrequencyPlot(Groceries,
topN=10,type="absolute")
rules<–apriori(Groceries, parameter = list(supp=0.1, conf=0.8), maxlen =5)
rules=apriori(Groceries, parameter = list(supp=0.1, conf=0.8), maxlen =5)
rules=apriori(Groceries, parameter = list(supp=0.01, conf=0.8), maxlen =5)
rules=apriori(Groceries, parameter = list(supp=0.01, conf=0.8))
rules=apriori(Groceries, parameter = list(supp=0.001, conf=0.8))
inspect(rules[1:15])
rules=sort(rules,by="confidence",decreasing=TRUE)
inspect(rules[1:15])
rules=apriori(Groceries, parameter = list(supp=0.001, conf=0.8),maxlenght=3)
rules=apriori(Groceries, parameter = list(supp=0.001, conf=0.8),maxlength=3)
rules=apriori(Groceries, parameter = list(supp=0.001, conf=0.8),maxlen=3)
rules=sort(rules,by="confidence",decreasing=TRUE)
inspect(rules[1:15])
setwd("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/m6 2017-8 - hp normali")
library("writexl")
install.packages("writexl")
library("writexl")
write_xlsx(pareto_aggregated,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/m6 2017-8 - hp normali/pareto_aggregated.xlsx")
pareto_aggregated<- read.csv(file = "pareto_aggregated.csv", header = TRUE)
index=which(pareto_aggregated$solID=="2_893_1")
pareto_aggregated=pareto_aggregated[index,]
write_xlsx(pareto_aggregated,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/pareto_aggregated2017-2019.xlsx)
write_xlsx(pareto_aggregated,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_aggregated2017-2019.xlsx)
write_xlsx(pareto_aggregated,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_aggregated2017-2019.xlsx")
setwd("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/6_2018-2020")
pareto_aggregated<- read.csv(file = "pareto_aggregated.csv", header = TRUE)
index=which(pareto_aggregated$solID=="2_877_1")
pareto_aggregated=pareto_aggregated[index,]
write_xlsx(pareto_aggregated,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_aggregated2018-2020.xlsx")
setwd("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/6_2019-2021")
pareto_aggregated<- read.csv(file = "pareto_aggregated.csv", header = TRUE)
index=which(pareto_aggregated$solID=="4_173_2")
index=which(pareto_aggregated$solID=="4_1732_2")
pareto_aggregated=pareto_aggregated[index,]
write_xlsx(pareto_aggregated,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_aggregated2019-2021.xlsx")
setwd("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/m6 2017-8 - hp normali")
pareto_alldecomp_matrix<- read.csv(file = "pareto_alldecomp_matrix.csv", header = TRUE)
write_xlsx(pareto_alldecomp_matrix,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_alldecomp_matrix2017-2019.xlsx")
setwd("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/6_2018-2020")
pareto_alldecomp_matrix<- read.csv(file = "pareto_alldecomp_matrix.csv", header = TRUE)
write_xlsx(pareto_alldecomp_matrix,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_alldecomp_matrix2018-2020.xlsx")
setwd("C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco/6_2019-2021")
pareto_alldecomp_matrix<- read.csv(file = "pareto_alldecomp_matrix.csv", header = TRUE)
write_xlsx(pareto_alldecomp_matrix,"C:/Users/User/OneDrive - Politecnico di Milano/5° year - 2° semenster/Analytics for Business LAB/Robyn/6_Marco\\pareto_alldecomp_matrix2019-2021.xlsx")
